<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>Joey Rucska - Data Science Projects</title>
<style>
  body {
    font-family: Tahoma, Arial, sans-serif;
    margin: auto;
  }

  /* Header */
  .header {
    padding: 40px;
    text-align: center;
    background: #ca672e;
    color: white;
    border-bottom: solid black;
    font-family: Georgia, serif;
  }
  .header h1 { font-size: 34px; }
  .header p  { font-size: 26px; }

  .top-link {
    text-decoration: none;
    background-color: rgba(86, 86, 86, 0.625);
    color: white;
    margin: 1rem;
    padding: 0.5rem 1rem;
    border-radius: 0.5rem;
    font-family: Tahoma, Arial, sans-serif;
    font-size: 22px;
    border: solid rgba(220, 220, 220, 0.625);
    /*display: inline-block;*/
  }
  .top-link:hover {
    color: black;
    background-color: rgba(220, 220, 220, 0.625);
    border: solid rgba(86, 86, 86, 0.625);
  }

  /* Content */
  .main {
    background-color: white;
    padding-top: 40px;
    padding-left: 25%;
    padding-right: 25%;
    line-height: 1.5;
  }
  .main h1 {
    padding-top: 16px;
    font-size: 20px;
    font-weight: bold;
    text-decoration: underline;
  }

    ul li {
            margin-bottom: 0.25lh; /* Adjust the value as needed */
    }

    ul {
            line-height: 1.5; /* Adjust the value for more or less space */
    }

  /* Plots section */
  .plots {
    background-color: white;
    padding-left: 20%;
    padding-right: 20%;
    line-height: 1.5;
    padding-bottom: 30px;
  }

  /* Controls */
  .controls {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
    margin-bottom: 10px;
    align-items: flex-end;
  }
  select, button {
    padding: 8px;
    font-size: 1rem;
  }

  .dropdown-block {
    display: inline-block;
    margin-right: 15px;
  }
  .dropdown-title {
    font-size: 16px;
    font-weight: 600;
    margin-bottom: 3px;
    text-align: left;
  }

  #analysis-description {
    background: #f8f8f8;
    padding: 10px 14px;
    border-left: 5px solid #3366cc;
    border-radius: 4px;
    margin-bottom: 15px;
  }

  /* Responsive plots layout (like Conditions.html) */
  .plots-wrapper {
    display: flex;
    flex-wrap: wrap;
    gap: 20px;
    justify-content: center;
  }

  .plot-container {
    text-align: center;
    flex: 1 1 400px;
  }

  #abs-container {
    flex: 1 1 100%;
    max-width: 630px;
    margin: 20px auto;
  }


  .plot-container h3 {
    margin-bottom: 0.5em;
  }
  img {
    max-width: 100%;
    height: auto;
    border: 1px solid #ddd;
    padding: 6px;
    margin-top: 10px;
  }

  /* For mobile: reduce padding / use full width */
  @media (max-width: 768px) {
    .main {
      padding-left: 40px;
      padding-right: 40px;
    }
    .plots {
      padding-left: 40px;
      padding-right: 40px;
    }
    img {
      max-width: 100%;
      width: 100%;
    }
  }

  /* Stack plots when too narrow to show side-by-side */
  @media (max-width: 850px) {
    .plots-wrapper { flex-direction: column; }
  }

  .hidden { display: none !important; }
</style>
</head>

<body>
  <div class="header">
    <h1>Building & assessing a "ratio-based" xG model for NHL shots</h1>
    <p>Joey Rucska</p>
    <p style="margin-bottom: 3rem;">Last updated on December 19, 2025.</p>
    <a href="https://github.com/rucskajj/Data-Science-Projects/tree/main/NHL/xG" class="top-link">Github</a>
  </div>

  <div class="main">
    <b>In this analysis, I develop a simple expected goals (xG) model based on one ratio: number of goals over number of shot attempts.</b> By determining this ratio for a specific sets of conditions: shot location, shot type, player position, etc., this ratio&mdash;in theory&mdash;becomes more representative of the true likelihood a given shot with those conditions will result in a goal. A good xG model is a useful tool for assessing a team's or individual's ability to drive offense. On this page, I describe how I develop and evaluate the performance of this model. <br><br>
    
    The Python code used for this analysis is available on my <a href="https://github.com/rucskajj/Data-Science-Projects/tree/main/NHL/xG">GitHub</a>. <br>
    <hr>

    <h1>Data & Procedure</h1>

    The dataset for this project comes from <a href="https://moneypuck.com/data.htm">MoneyPuck</a>. There are approx. 100,000 shot "events" in any given season's worth of data, and for my models I use data from seasons inclusive of 2015-2024. I discuss the quality of this data set in a <a href="https://rucskajj.github.io/Data-Science-Projects/NHL/Conditions/Conditions.html">previous section</a> of this analysis. The goal of that section is get an overall glimpse of the data set, and to identify which conditions have the strongest influence on shot outcomes (goal/no goal), which informs the development of my xG model here. From that analysis, I have identified the following conditions/factors as the most important, where (B) is a binary variable and (M) is a multi-value variable:
    <ul>
        <li><b>Regular season vs. playoffs</b> (B). Did this shot occur during a regular season or playoff game?</li>
        <li><b>Rebounds</b> (B). Did this shot occur during a rebound?</li>
        <li><b>Forward vs. Defence</b> (B). Did this shot occur during a shot from a forward or defence?</li>
        <li><b>Shot type</b> (M). 7 available options: wrist shot, snap shot, slap shot, backhand, deflection, tip-in, wraparound.</li>
        <li><b>Playing strength</b> (M). 7 available options: 5v5, 5v4, 4v5, 4v4, 3v3, 5v3, 6v5.</li>
    </ul>

    With these insights in hand, my procedure for building & testing my "ratio-based" xG model is as follows:
    <ol type="1">
        <li>Split all data into two data sets: 1) a "<b>testing</b>"" data set comprising one season's worth of shots 2) a "<b>training</b>" data set, which all shots from all other seasons. (I anticipate building a follow-up analysis based on machine learning models, hence the "testing"/"training" language.)</li>
        <li>For the <b>training</b> set, filter the data according to a specific set of condition values. For example, a set using all conditions could be: (Regular season game; non-rebound; forward player; wrist shot; 5v5). For all shot attempts belonging to this set of conditions, calculate (and save) a 2D histogram of xG = num. goals/num. shot attempts. (This histogram is in distance-angle space, and is the <u>same idea</u> as the greyscale xG histograms presented in the <a href="https://rucskajj.github.io/Data-Science-Projects/NHL/Conditions/Conditions.html">previous section</a> of my analysis.)</li>
        <li><u>For all unique combinations of condition-value pairs</u>, repeat the previous step. This calculates (and saves) a unique xG histogram for each set of condition-value pairs.</li>
        <li>For every shot attempt/record in the <b>testing</b> set, retrieve the appropriate xG value from the applicable 2D histogram. That is, 1) identify the conditions for this shot (rebound? F/D? etc.) and load that histogram, 2) identify the appropriate bin in that histogram according to the location of that shot. The xG value in that bin is the xG value for that shot.</li>
        <li>Use all xG values for all shots in the <b>testing</b> set to assess the performance of the model. This is discussed in detail below.</li>
    </ol>

    Note: I repeat these steps using different seasons for the testing data, to see how the performance varies depending on the individual season that is chosen.<br><br>

    I was curious to see how influential these conditionals truly are. So, I built three "different" models, which consider a different set of conditions, while ignoring the rest:
    <ul>
        <li><b>Model 1</b>: all conditions are included (392 unique condition sets, i.e. xG histograms).</li>
        <li><b>Model 2</b>: all conditions except for the regular season vs. playoffs condition are included (196 condition sets).</li>
        <li><b>Model 3</b>: only the rebounds and forward vs. defence conditions are included (4 condition sets).</li>
    </ul>

    <h1>Analysis Methods</h1>

    To assess the ability of these models to predict goals, I use three metrics (which are presented on the y-axes of the plots below):
    <ul>
        <li><b>Log loss</b>: a standard measure of model performance, i.e. how similar a model prediction is to an "observed" outcome. This metric is commonly used to assess logistic regression-based machine learning models. In my analysis, for each shot/record, a log loss value is calculated, then the mean log loss value across all shots is calculated for each model. The equation I use is below, where "g" is the outcome of the shot; g = 1 for a goal, g = 0 for no goal. "xG" is the expected goals or xG value for that shot. Log loss is low when the prediction is similar to the outcome, and is exactly zero when g=1 and xG=1 or g=0 and xG=0, as these represent "perfect" predictions. Based on the xG histograms from the <a href="https://rucskajj.github.io/Data-Science-Projects/NHL/Conditions/Conditions.html">previous section</a>, xG values for a set of conditions typically peak around 0.2.<p><img src="./images/logloss-equation.png" alt="An image showing the equation used for logloss." style="border:none; display:block; margin: 0 auto; width:450px"></p></li>
        <li><b>Goal difference</b>: the difference between the <b>predicted</b> number of goals and the <b>observed</b> number of goals in the <b>testing</b> data set. The predicted number is exactly equal to the sum all of xG values. The observed number is simply a count of the number of goals that actually occurred (recall, the outcome of each shot is available: SOG, miss, goal).</li>
        <li><b>Goal difference (%)</b>: the same difference above, expressed as percent where the <u>observed goals</u> is the denominator.</li>
    </ul>

    You can pick which metric plot you want to see via the "Metric" drop-down. <br><br>

    The plots I present are as follows:
    <ul>
        <li><b>All models</b>: A plot of the chosen metric for all three models against the minimum number of events/shot attempts (SAT) in the xG bins. While looping through all shots in the testing data, I sorted the pulled xG values according to various (arbitrary) thresholds for the minimum number of records/shots in that bin. The side-length of the square markers is proportional to the ratio of the records/shots included at that minimum number of events. The empty square is the maximum possible square size. The lines are the mean values across all analyses with different choices for the training data set (i.e. a "horizontal" average of the "single model" plots); shaded regions denote one standard deviation above and below the mean.</li>
    </ul>
    <ul>
        <li><b>Single model</b>: A plot of the chosen metric for an individual models against the season which was used as the testing data. Individual lines track this metric according to different values for the minimum event threshold (see point above).</li>
        <li><b>|Goal Diff| (%)</b>: The same data as the lines for the "Metric"="Goal difference %", with an absolute value applied and shaded regions removed.</li>
    </ul>

    Lastly, I also present two different analyses which use a different base data set. One analysis uses the data for all 10 seasons inclusive of 2015-2024, while the other analysis uses a smaller data set comprised of the 5 seasons inclusive of 2020-2024.<br>

    <h1>Results & Interpretation</h1>

    <!-- Data set selector -->
    <div class="controls">
      <label for="analysis-select"><b>Analysis:</b></label>
      <select id="analysis-select"></select>
    </div>

    <div id="analysis-description"><!-- Analysis description will appear here --></div>

    <!-- Dropdowns: metric applies to BOTH plots; model applies to single-model plot -->
    <div class="controls">
      <div class="dropdown-block">
        <div class="dropdown-title" id="metric-title">Metric</div>
        <select id="metric-select"></select>
      </div>

      <div class="dropdown-block" id="model-block">
        <div class="dropdown-title" id="model-title">Model</div>
        <select id="model-select"></select>
      </div>
    </div>

    <!-- Toggle buttons (same location) -->
    <div class="controls">
      <p style="font-size: 18px; margin-top: 0.5em; margin-bottom: 0.5em;">Toggle show/hide:</p>
      <button id="toggle-agg" type="button">All models</button>
      <button id="toggle-single" type="button">Single model</button>
      <button id="toggle-abs" type="button">|Goal diff.| (%)</button>
    </div>
  </div>

  <div class="plots">
    <!-- Two side-by-side plot containers -->
    <div class="plots-wrapper">
      <div class="plot-container" id="agg-container">
        <h3 id="agg-title">All models</h3>
        <img id="agg-img" src="" alt="Aggregate plot" />
        <div class="controls" style="justify-content:center; margin-top:10px;">
          <button id="download-agg" type="button">Download all models plot</button>
        </div>
      </div>

      <div class="plot-container" id="single-container">
        <h3 id="single-title">Single model</h3>
        <img id="single-img" src="" alt="Single model plot" />
        <div class="controls" style="justify-content:center; margin-top:10px;">
          <button id="download-single" type="button">Download single model plot</button>
        </div>
      </div>
    </div>

    <!-- Special-case plot container (separate) -->
    <div class="plot-container" id="abs-container" style="margin-top: 20px;">
      <h3 id="abs-title">Absolute value of Goal differential (%)</h3>
      <img id="abs-img" src="" alt="Aggregate |xG − Observed| % plot" />
      <div class="controls" style="justify-content:center; margin-top:10px;">
        <button id="download-abs" type="button">Download |Goal diff.| (%) plot</button>
      </div>
    </div>
  </div>

<script>
/* ---------------- Labels ---------------- */

const analysisLabels = {
  "2015-2024": "2015-2024",
  "2020-2024": "2020-2024"
};

const analysisDescriptions = {
  "2015-2024": `
    <b>These plots & results are come from an analysis based on 10 seasons worth of data, from the years 2015-2024 (inclusive).</b> Explicitly, the <b>training</b> data is built off 9 seasons of data, while 1 season is reserved for <b>testing</b>. Thus, 10 different "rounds" of xG models were produced, where each of the 10 seasons individually "took turns" as the testing data. The All Models plots present statistical averages/summaries across the 10 "rounds", while the Single model displays the performance of each "round". <br><br>

    The primary utility of log loss here is to compare the models to each other. Recall: low log loss is an indicator of strong model performance. At the moment, it is unclear to me whether 0.2 as an average value can be interpreted as "good" in a vacuum, and similarly whether decreases of 0.1 or 0.2 in this value are substantial. Some specific comments on the <b>log loss</b> plots:
    <ul>
      <li>Looking at the All models plot, the most obvious trend is that (for Models 1 and 2), log loss decreases as we increase the minimum threshold for shot attempts in a bin. This is expected: restricting yourself to only using the model when there is a high number of samples should improve the performance of the model. With a higher the number of samples, the ratio the model is based off of becomes increasingly representative of the true likelihood shots under those conditions result in goals.</li>
      <li>Of course, applying these kinds of restrictions means your model is ignoring or "giving up" on attempting to predict more fringe scenarios, where shots are apparently less likely to occur. The side length of the squares in this plot represents the proportion of the data set covered by the model under these restrictions. For a threshold of 100 SAT per bin, model 1 covers ~85% of the data set.</li>
      <li>The trend of decreasing log loss with minimum SAT threshold does not apply to Model 3, however. This model is a very minimalistic, and attempts to make predictions based on only three conditions: shot location (with ~140 bins in distance-angle space), and two binary markers, rebounds & forward vs. defensive shooters. The performance of this model does not increase with SAT threshold, since there are so few conditions to split the data set, so the coverage mentioned in the previous point (tracked by the size of the squares) is fairly constant.</li>
      <li>Looking at the single models plot, we see that there is substantial variation in log loss both in terms of 1) which season was used as the testing data (i.e. horizontally) and 2) the minimum SAT threshold, tracked by lines of different shades. The end-to-end horizontal variation in log loss is of order ~0.1, which is similar to the decrease in log loss going from a threshold of 10 to 100 for Models 1 & 2. I don't care so much that log loss increases vs. decreases, but that there is a <u>consistent, monotonic trend</u>. Professional hockey is dynamic; strategies are constantly evolving. This trend could be a sign that there is a real difference between how shot outcome behaves across the 2015-2025 decade. This result inspired my other analyses which focus on a narrower range of data.</li>
    </ul>

    Goal difference is a more intuitive metric for model performance. It directly compares the number of goals predicted by the model (as a simple sum of likelihoods, i.e. xG) to the number of actual or "observed" goals present in the testing data set. Some specific comments on the <b>Goal diff</b> plots:
    <ul>
        <li>In the All models plots, like with log loss, there appears to be a minimal difference between the performance of Model 1 and Model 2. For the non-percentage difference, all three models perform similarly, but the in the percentage plot, Model 3 appears to perform better, as its shaded region (representing a standard deviation) is narrower. The only trend with SAT threshold is in the percentage plots: the spread in outcomes for Model 1 and Model 2 increases.</li>
        <li>The averages values (solid lines) look quite convincing, hovering near zero. Indeed, the |Goal diff.| (%) plot shows these values are all less than 1%! However, this average is masking a very strong dependence with what data is used for the testing set, as seen in the Single model plots. There was a similar dependence seen in the log loss plots, which, as I discuss above, could be an indicator in true variation in the behaviour of shot outcomes across this decade.</li>
    </ul>

    So, what can we take away from this? Are these models good at making predictions? Which is best? To me, the easiest place to start is Goal diff %: in any individual "round" (single testing season), all models are able to predict the number of goals in an entire season to within 5-10%. Perhaps that is good enough, it depends on your tolerance. Model 3 performs best in Goal differential, but worse in log loss, which is an indicator that, on an individual, shot-to-shot basis, Model 3 is not as predictive as Model 1 or 2 (when these models are limited to bins with >50 events in them).<br><br>

    The strong, monotonic dependence on these metrics with the chosen <b>testing</b> season is alarming to me, and an indicator that the <b>training</b> data set is perhaps too large in scope. There may be too much intrinsic variation in system behaviour across this decade to compile all of these data together. Clearly, testing the 2015-'16 & '16-'17 leads to the same behaviour, while testing the '22-'23 and '23-24' leads to opposite behaviour. This motivates my second Analysis, where I narrow the scope to 2020-2024 data, which I direct the interested reader to now.
  `,
  "2020-2024": `
    <b>These plots & results are come from an analysis based on 5 seasons worth of data, from the years 2020-2024 (inclusive).</b> Explicitly, the <b>training</b> data is built off 4 seasons of data, while 1 season is reserved for <b>testing</b>. Thus, 5 different "rounds" of xG models were produced, where each of the 5 seasons individually "took turns" as the testing data. The All Models plots present statistical averages/summaries across the 5 "rounds", while the Single model displays the performance of each "round".<br><br>

    There are similar, general observations regarding these plots which apply to both analyses, such as how log loss decreases and the spread in Goal diff (%) increases with increasing SAT threshold. I will not repeat those discussions here, but instead focus on the meaningful <u>differences</u> between these results and the results from the 2015-2024 analysis. 
    <ul>
      <li>Most importantly, the monotonic, "horizontal" trends in the single model plots has disappeared. This is true for both log loss and Goal diff. Ideally, if shot outcome predictors were consistent from year-to-year, the model should identically no matter which season is used at the testing data. This analysis is much closer to that truth than the 2015-2024 analysis.</li>
      <li>Looking at the All models plots, the spread in log loss (signified by the shaded regions) is much narrower than in the 2015-2024 analysis. There is not a significant difference in how Model 1 and Model 2 performs for SAT thresholds of 50 and 100, for example.</li>
    </ul>

    I believe my most substantial results thus far is discovering the intrinsic variation in shot outcomes across the 2015-2024 decade. Using this statistical, ratio-based technique, it is tempting to assume that increasing the amount of underlying "<b>training</b>" or reference data will produce better results. Of course, this is only true if the system behaviour is consistent across that data set! Here, I have cut the reference data set in half, and seen an <u>improvement</u> in model performance/consistency.<br><br>
    
    There is an obvious question regarding what exactly is so different from the 2015 and 2024 shot behaviour. Did teams start prioritizing some shots over others? Surely, in some way, this data set is capable of addressing that question. For now, I will leave that study for another time. I am more interested in how to build good xG models at the present.<br><br>

    So, are <u>these</u> models "good"? The most logical direction for future work is to test these models against other methods, such as machine learning techniques. Indeed, I've been planning this development from the outset of this project, hence my persistent use of "training" and "test" language. Perhaps comparing the performance of ML models can provide insights to whether this technique based on careful filtering, binning, and a simple ratio is enough to reliably model NHL shot outcome behaviour.
  `
};

/* Metric order / labels */
const metricOrder = ["avglogloss", "xG_obs_diff", "xG_obs_diff_pct"];
const metricLabels = {
  "avglogloss": "Log loss",
  "xG_obs_diff": "Goal difference",
  "xG_obs_diff_pct": "Goal difference (%)"
};

/* Special-case (separate plot container) */
const absMetricKey = "abs_xG_obs_diff_pct";
const absMetricLabel = "|xG − Observed| %";

const modelLabels = {
  "1": "Model 1",
  "2": "Model 2",
  "3": "Model 3"
};

/* ---------------- DOM ---------------- */

const analysisSel = document.getElementById("analysis-select");
const metricSel = document.getElementById("metric-select");
const modelSel = document.getElementById("model-select");

const aggImg = document.getElementById("agg-img");
const singleImg = document.getElementById("single-img");
const absImg = document.getElementById("abs-img");

const aggTitle = document.getElementById("agg-title");
const singleTitle = document.getElementById("single-title");
const absTitle = document.getElementById("abs-title");

const aggContainer = document.getElementById("agg-container");
const singleContainer = document.getElementById("single-container");
const absContainer = document.getElementById("abs-container");

const toggleAggBtn = document.getElementById("toggle-agg");
const toggleSingleBtn = document.getElementById("toggle-single");
const toggleAbsBtn = document.getElementById("toggle-abs");

const downloadAggBtn = document.getElementById("download-agg");
const downloadSingleBtn = document.getElementById("download-single");
const downloadAbsBtn = document.getElementById("download-abs");

/* ---------------- Options from filesystem ----------------
Based on xGRat-filestructure.txt:
- plots/<analysis>/{avglogloss,xG_obs_diff,xG_obs_diff_pct,abs_xG_obs_diff_pct}.png
- plots/<analysis>/singlemodel/model-<1|2|3>-{avglogloss,xG_obs_diff,xG_obs_diff_pct}.png
*/

/* Available analyses (directories under plots/) */
const analyses = ["2015-2024", "2020-2024"];

/* ---------------- Helpers ---------------- */

function populateSelect(selectEl, options, labelsDict) {
  selectEl.innerHTML = "";
  options.forEach(opt => selectEl.add(new Option(labelsDict?.[opt] ?? opt, opt)));
}

function updateAnalysisDescription() {
  const descElem = document.getElementById("analysis-description");
  const key = analysisSel.value;
  descElem.innerHTML = analysisDescriptions[key] || "";
}

function buildAggregatePath(analysisKey, metricKey) {
  return {
    src: `plots/${analysisKey}/${metricKey}.png`,
    //title: `${analysisLabels[analysisKey] || analysisKey}: Aggregate — ${(metricLabels[metricKey] || metricKey)}`
    title: `All models (${analysisLabels[analysisKey] || analysisKey})`
  };
}

function buildSingleModelPath(analysisKey, modelKey, metricKey) {
  return {
    src: `plots/${analysisKey}/singlemodel/model-${modelKey}-${metricKey}.png`,
    //title: `${analysisLabels[analysisKey] || analysisKey}: ${modelLabels[modelKey] || ("Model " + modelKey)} — ${(metricLabels[metricKey] || metricKey)}`
    title: `${modelLabels[modelKey] || ("Model " + modelKey)} (${analysisLabels[analysisKey] || analysisKey})`
  };
}

function buildAbsPath(analysisKey) {
  return {
    src: `plots/${analysisKey}/${absMetricKey}.png`,
    title: `${analysisLabels[analysisKey] || analysisKey}: Aggregate — ${absMetricLabel}`
  };
}

function updatePlots() {
  const analysisKey = analysisSel.value;
  const metricKey = metricSel.value;
  const modelKey = modelSel.value;

  const agg = buildAggregatePath(analysisKey, metricKey);
  const single = buildSingleModelPath(analysisKey, modelKey, metricKey);
  const abs = buildAbsPath(analysisKey);

  aggImg.src = agg.src;
  singleImg.src = single.src;
  absImg.src = abs.src;

  // Uncomment to have dynamic titles based on the chosen drop-down values
  aggTitle.textContent = agg.title;
  singleTitle.textContent = single.title;
  /*absTitle.textContent = abs.title;*/
}

/* ---------------- Event listeners ---------------- */

analysisSel.addEventListener("change", () => {
  updateAnalysisDescription();
  updatePlots();
});

metricSel.addEventListener("change", updatePlots);
modelSel.addEventListener("change", updatePlots);

/* toggles */
toggleAggBtn.addEventListener("click", () => aggContainer.classList.toggle("hidden"));
toggleSingleBtn.addEventListener("click", () => singleContainer.classList.toggle("hidden"));
toggleAbsBtn.addEventListener("click", () => absContainer.classList.toggle("hidden"));

/* downloads */
function downloadCurrent(src) {
  if (!src) return;
  const a = document.createElement("a");
  a.href = src;
  a.download = src.split("/").pop();
  a.click();
}
downloadAggBtn.addEventListener("click", () => downloadCurrent(aggImg.src));
downloadSingleBtn.addEventListener("click", () => downloadCurrent(singleImg.src));
downloadAbsBtn.addEventListener("click", () => downloadCurrent(absImg.src));

/* ---------------- Start page ---------------- */

populateSelect(analysisSel, analyses, analysisLabels);
populateSelect(metricSel, metricOrder, metricLabels);
populateSelect(modelSel, ["1","2","3"], modelLabels);

updateAnalysisDescription();
updatePlots();

</script>
</body>
</html>
